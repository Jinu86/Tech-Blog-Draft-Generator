import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Gemini API í‚¤ ì„¤ì •
genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
REACT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ê¸°ìˆ  ë¸”ë¡œê·¸ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ReAct ë°©ì‹(Reasoning + Acting)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ìì˜ ì…ë ¥ì„ í•´ì„í•˜ê³ , ë‹¹ì‹ ì´ ì´í•´í•œ ë‚´ìš©ì´ ë§ëŠ”ì§€ ë‹¤ì‹œ ë¬¼ì–´ë³¸ í›„ ì‚¬ìš©ì í™•ì¸ì„ ë°›ì€ ë‹¤ìŒì—ë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.

ì‚¬ìš©ì ì…ë ¥ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆëª…í™•í•  ê²½ìš° ë°˜ë“œì‹œ ëª…í™•í•˜ê²Œ ë‹¤ì‹œ ë¬¼ì–´ë³´ì„¸ìš”.
ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì§€ ë§ê³ , ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.

ì§„í–‰ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. [ì£¼ì œ íŒŒì•…]
2. [í‚¤ì›Œë“œ ì¶”ì²œ ë° ì„ íƒ]
3. [ë¬¸ì²´/ëŒ€ìƒ ìŠ¤íƒ€ì¼ ì„ íƒ]
4. [ê¸€ì˜ êµ¬ì¡° ì œì•ˆ ë° í™•ì •]
5. [ì†Œì œëª© ë° íë¦„ êµ¬ì„±]
6. [ì´ˆì•ˆ ì‘ì„±]

ê° ë‹¨ê³„ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŒ¨í„´ì„ ë”°ë¥´ì„¸ìš”:
- ğŸ§ Reasoning: ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¹ì‹ ì´ ì´í•´í•œ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
- âš™ï¸ Acting: ì´í•´í•œ ë‚´ìš©ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ê³ , ë§ëŠ”ì§€ ë¬¼ì–´ë´…ë‹ˆë‹¤.
- âœ… ì‚¬ìš©ì í™•ì¸ ì´í›„ì—ë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”.
"""

# ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
PROMPT_TOPIC_QUESTION = """
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ê¸°ìˆ  ë¸”ë¡œê·¸ ì´ˆì•ˆ ì‘ì„±ì„ ë„ì™€ë“œë¦¬ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ğŸ˜Š
ë¨¼ì €, ì–´ë–¤ ì£¼ì œë¡œ ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
ê°„ë‹¨íˆ ë§ì”€í•´ ì£¼ì„¸ìš”.
"""

PROMPT_TOPIC_CONFIRM = """
ğŸ§ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì œê°€ ì´í•´í•œ ì£¼ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  
**"{inferred_topic}"**

âš™ï¸ ì´ ì£¼ì œë¡œ ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ì‹œëŠ” ê²Œ ë§ì„ê¹Œìš”?
ë§ìœ¼ë©´ \"ë„¤\", ì•„ë‹ˆë©´ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.
"""

PROMPT_KEYWORD_QUESTION = """
ì£¼ì œ \"**{topic}**\"ì™€ ê´€ë ¨í•´ì„œ ì•„ë˜ì™€ ê°™ì€ í‚¤ì›Œë“œë¥¼ ì¶”ì²œë“œë ¤ìš”:

ğŸ” ì¶”ì²œ í‚¤ì›Œë“œ:
{recommended_keywords}

ì´ ì¤‘ì—ì„œ ë‹¤ë£¨ê³  ì‹¶ì€ í‚¤ì›Œë“œë¥¼ **ë³µìˆ˜ë¡œ ì„ íƒ**í•´ì£¼ì‹œê³ ,
ì¶”ì²œ í‚¤ì›Œë“œì— ì—†ë”ë¼ë„ ì¶”ê°€í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ììœ ë¡­ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”!
ì˜ˆ: \"API, Mock ì„œë²„, ì‹¤ìŠµ ì˜ˆì œ\"
"""

PROMPT_KEYWORD_CONFIRM = """
ğŸ§ ì œê°€ ì´í•´í•œ ìµœì¢… í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  
{selected_keywords}

âš™ï¸ ì´ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê¸€ì„ ì‘ì„±í•´ë„ ê´œì°®ì„ê¹Œìš”?
ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”!
"""

PROMPT_STYLE_QUESTION = """
ì´ë²ˆì—” ë¸”ë¡œê·¸ì˜ ìŠ¤íƒ€ì¼ì„ ì •í•´ë³¼ê²Œìš”.
ì•„ë˜ëŠ” ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤:

- í˜•ì‹: íŠœí† ë¦¬ì–¼, ê¸°ìˆ  ë¦¬ë·°, ë¬¸ì œ í•´ê²° ì‚¬ë¡€
- ë¬¸ì²´: ì¹œê·¼í•œ, ê³µì‹ì ì¸, ì¤‘ë¦½ì 
- ë…ì ëŒ€ìƒ: ì´ˆë³´ì, ì¤‘ê¸‰ ê°œë°œì, ì „ë¬¸ê°€

ì˜ˆì‹œì—ì„œ ê³¨ë¼ë„ ì¢‹ê³ , ììœ ë¡­ê²Œ ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•´ì£¼ì…”ë„ ê´œì°®ìŠµë‹ˆë‹¤.
ì˜ˆ: \"íŠœí† ë¦¬ì–¼ í˜•ì‹, ì¹œê·¼í•œ í†¤, ì´ˆë³´ì ëŒ€ìƒ\"
"""

PROMPT_STYLE_CONFIRM = """
ğŸ§ ì œê°€ ì´í•´í•œ ìŠ¤íƒ€ì¼ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- í˜•ì‹: **{format_style}**
- ë¬¸ì²´: **{tone}**
- ëŒ€ìƒ ë…ì: **{audience}**

âš™ï¸ ì´ ìŠ¤íƒ€ì¼ë¡œ ê¸€ì„ ì‘ì„±í•´ë„ ê´œì°®ì„ê¹Œìš”?
ììœ ë¡­ê²Œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•˜ê³  ì‹¶ì€ ìš”ì†Œê°€ ìˆë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”.
"""

PROMPT_STRUCTURE_SUGGEST = """
ìœ„ì˜ ì£¼ì œ, í‚¤ì›Œë“œ, ìŠ¤íƒ€ì¼ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ê¸€ êµ¬ì¡°ë¥¼ ì œì•ˆë“œë ¤ìš”:

ğŸ“ ì œì•ˆëœ êµ¬ì¡°:
{suggested_structure}

âš™ï¸ ì´ êµ¬ì¡°ëŠ” ì°¸ê³ ìš©ì´ë‹ˆ, ë§ˆìŒê» ìˆ˜ì •í•˜ì…”ë„ ì¢‹ì•„ìš”!
ì„¹ì…˜ì„ ì¶”ê°€í•˜ê±°ë‚˜ ìˆœì„œë¥¼ ë°”ê¾¸ê³  ì‹¶ìœ¼ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.
"""

PROMPT_SUBTITLES_CONFIRM = """
ì•„ë˜ëŠ” ê° ì„¹ì…˜ì˜ ì†Œì œëª©ì…ë‹ˆë‹¤:

ğŸ“Œ ì†Œì œëª© ëª©ë¡:
{finalized_subtitles}

âš™ï¸ ì´ íë¦„ëŒ€ë¡œ ê¸€ì„ ì‘ì„±í•´ë„ ê´œì°®ì„ê¹Œìš”?
ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•˜ê³  ì‹¶ì€ í•­ëª©ì´ ìˆë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”!
"""

PROMPT_DRAFT_SECTION = """
âœï¸ ì„¹ì…˜ \"**{section_title}**\"ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ì´ˆì•ˆì„ ì‘ì„±í•´ë´¤ì–´ìš”:

```
{generated_content}
```

âš™ï¸ ê´œì°®ìœ¼ì‹ ê°€ìš”?
ë‚´ìš©ì„ ë°”ê¾¸ê³  ì‹¶ê±°ë‚˜, ë” ì¶”ê°€í•˜ê³  ì‹¶ì€ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”!
"""

# Gemini ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
def get_chat_model():
    return genai.GenerativeModel("gemini-pro-1.5")

# ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.step = "topic_question"
    st.session_state.collected = {}
    st.session_state.generated_drafts = {}
    st.session_state.draft_index = 0

# ì‚¬ì´ë“œë°” ì§„í–‰ ë‹¨ê³„ í‘œì‹œ
with st.sidebar:
    st.markdown("### ğŸ§­ ì§„í–‰ ë‹¨ê³„")
    steps = [
        ("topic", "1. ì£¼ì œ ì…ë ¥"),
        ("keyword", "2. í‚¤ì›Œë“œ ì„ íƒ"),
        ("style", "3. ìŠ¤íƒ€ì¼ ì„¤ì •"),
        ("structure", "4. êµ¬ì¡° ì œì•ˆ"),
        ("subtitle", "5. ì†Œì œëª© êµ¬ì„±"),
        ("draft", "6. ì´ˆì•ˆ ì‘ì„±")
    ]
    current_step = st.session_state.step
    for key, label in steps:
        if current_step.startswith(key):
            st.markdown(f"- **âœ… {label}**")
        else:
            st.markdown(f"- {label}")

# ì±— UI
st.title("ğŸ§  ê¸°ìˆ  ë¸”ë¡œê·¸ ì´ˆì•ˆ ìƒì„± ì±—ë´‡")
st.markdown("---")

# ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì±—ë´‡ ë©”ì‹œì§€ ì „ì†¡ í•¨ìˆ˜
def bot_say(message):
    st.session_state.messages.append({"role": "assistant", "content": message})
    with st.chat_message("assistant"):
        st.markdown(message)

# ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜
def user_say():
    user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)
        handle_input(user_input)

# ë‹¨ê³„ë³„ ì²˜ë¦¬ ë¡œì§
def handle_input(user_input):
    step = st.session_state.step
    model = get_chat_model()

    if step == "topic_question":
        st.session_state.step = "topic_confirm"
        st.session_state.collected["user_topic"] = user_input

        prompt = f"""
{REACT_SYSTEM_PROMPT}

ì‚¬ìš©ì ì…ë ¥: "{user_input}"

ìœ„ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ìˆ  ë¸”ë¡œê·¸ ì£¼ì œë¥¼ ì¶”ë¡ í•´ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ë‹¤ìŒ ì§ˆë¬¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
\n\nì˜ˆì‹œ: \nğŸ§  ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì œê°€ ì´í•´í•œ ì£¼ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: \"Postman ì‚¬ìš©ë²•\"\nâš™ï¸ ì´ ì£¼ì œë¡œ ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ì‹œëŠ” ê²Œ ë§ì„ê¹Œìš”?
"""
        response = model.generate_content(prompt)
        inferred_text = response.text
        bot_say(inferred_text)

    elif step == "topic_confirm":
        if "ë„¤" in user_input:
            bot_say("ì¢‹ì•„ìš”! ì´ì œ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì²œë“œë¦´ê²Œìš”.")
            st.session_state.step = "keyword_question"
        else:
            bot_say("ì£¼ì œë¥¼ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            st.session_state.step = "topic_question"

    elif step == "keyword_question":
        st.session_state.collected["user_keywords_raw"] = user_input
        # ì´ê³³ì— í‚¤ì›Œë“œ ì¶”ì²œ ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŒ
        st.session_state.step = "keyword_confirm"
        prompt = f"ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œ í›„ë³´: {user_input}\nìœ„ ë‚´ìš©ì„ ì •ë¦¬í•˜ì—¬ GPTê°€ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì œí•˜ê³ , í™•ì¸ ì§ˆë¬¸ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±"
        response = model.generate_content(prompt)
        bot_say(response.text)

    elif step == "keyword_confirm":
        if "ë„¤" in user_input:
            bot_say("ìŠ¤íƒ€ì¼ì„ ì •í•´ë³¼ê²Œìš”. ë¬¸ì²´ì™€ ëŒ€ìƒ ë…ìë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.")
            st.session_state.step = "style_question"
        else:
            bot_say("ë‹¤ì‹œ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.session_state.step = "keyword_question"

    elif step == "style_question":
        st.session_state.collected["user_style_raw"] = user_input
        st.session_state.step = "style_confirm"
        prompt = f"ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìŠ¤íƒ€ì¼: {user_input}\ní˜•ì‹, í†¤, ë…ìëŒ€ìƒì„ ì •ë¦¬í•˜ì—¬ í™•ì¸ ë©”ì‹œì§€ ìƒì„±"
        response = model.generate_content(prompt)
        bot_say(response.text)

    elif step == "style_confirm":
        if "ë„¤" in user_input:
            bot_say("ì¢‹ì•„ìš”! ì´ì œ ê¸€ì˜ êµ¬ì¡°ë¥¼ ì œì•ˆë“œë¦´ê²Œìš”.")
            st.session_state.step = "structure_suggest"
        else:
            bot_say("ìŠ¤íƒ€ì¼ì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.session_state.step = "style_question"

    elif step == "structure_suggest":
        prompt = f"ì£¼ì œì™€ í‚¤ì›Œë“œ, ìŠ¤íƒ€ì¼ì„ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. ê° ì„¹ì…˜ì€ ì œëª©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”."
        response = model.generate_content(prompt)
        st.session_state.collected["suggested_structure"] = response.text
        st.session_state.step = "structure_confirm"
        bot_say(response.text + "\n\nì´ êµ¬ì¡°ë¡œ ê´œì°®ì„ê¹Œìš”?")

    elif step == "structure_confirm":
        if "ë„¤" in user_input:
            bot_say("ì¢‹ìŠµë‹ˆë‹¤. ì´ì œ ê° ì†Œì œëª©ì„ í™•ì •í•˜ê² ìŠµë‹ˆë‹¤.")
            st.session_state.step = "subtitle_confirm"
        else:
            bot_say("ì›í•˜ì‹œëŠ” êµ¬ì¡°ë¥¼ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
            st.session_state.step = "structure_suggest"

    elif step == "subtitle_confirm":
        st.session_state.collected["finalized_subtitles"] = [s.strip() for s in user_input.split("\n") if s.strip()]
        st.session_state.step = "draft_generate"
        st.session_state.draft_index = 0
        bot_say("ì´ì œ ê° ì„¹ì…˜ë³„ë¡œ ì´ˆì•ˆì„ ì‘ì„±í•´ë“œë¦´ê²Œìš”!")
        handle_input("")  # ìë™ìœ¼ë¡œ ì²« ì„¹ì…˜ ì´ˆì•ˆ ìƒì„± ì‹œì‘

    elif step == "draft_generate":
        subtitles = st.session_state.collected.get("finalized_subtitles", [])
        index = st.session_state.draft_index

        if index >= len(subtitles):
            st.session_state.step = "done"
            bot_say("âœ… ëª¨ë“  ì´ˆì•ˆ ì‘ì„±ì„ ì™„ë£Œí–ˆì–´ìš”! í•„ìš”í•œ ê²½ìš° ë‹¤ì‹œ ìˆ˜ì •í•˜ê±°ë‚˜ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.")
            return

        current_section = subtitles[index]
        prompt = f"{REACT_SYSTEM_PROMPT}\n\nì£¼ì œ: {st.session_state.collected.get('user_topic')}\në¬¸ì²´/ìŠ¤íƒ€ì¼: {st.session_state.collected.get('user_style_raw')}\n\nâœï¸ ì•„ë˜ ì†Œì œëª©ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\nì œëª©: {current_section}"
        
        try:
            response = model.generate_content(prompt)
            st.session_state.generated_drafts[current_section] = response.text
            st.session_state.step = "draft_confirm"
            bot_say(f"âœï¸ ì„¹ì…˜ \"**{current_section}**\"ì˜ ì´ˆì•ˆì…ë‹ˆë‹¤:\n\n{response.text}\n\nì´ ë‚´ìš© ê´œì°®ìœ¼ì‹ ê°€ìš”? ìˆ˜ì •í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‘ì„±í•˜ê³  ì‹¶ìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            bot_say(f"ì´ˆì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.session_state.step = "subtitle_confirm"

    elif step == "draft_confirm":
        if "ë„¤" in user_input:
            st.session_state.draft_index += 1
            st.session_state.step = "draft_generate"
            handle_input("")  # ë‹¤ìŒ ì„¹ì…˜ ìë™ í˜¸ì¶œ
        else:
            current_section = list(st.session_state.generated_drafts.keys())[st.session_state.draft_index]
            prompt = f"ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì œëª©: {current_section}, ì‚¬ìš©ì ìš”ì²­: {user_input}"
            
            try:
                response = model.generate_content(prompt)
                st.session_state.generated_drafts[current_section] = response.text
                bot_say(f"ğŸ” ë‹¤ì‹œ ì‘ì„±í•œ ì´ˆì•ˆì…ë‹ˆë‹¤:\n\n{response.text}\n\nì´ì œ ê´œì°®ìœ¼ì‹ ê°€ìš”?")
            except Exception as e:
                bot_say(f"ì´ˆì•ˆ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì²« ì§ˆë¬¸ í‘œì‹œ
if not st.session_state.messages:
    bot_say(PROMPT_TOPIC_QUESTION)

# ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
user_say()
